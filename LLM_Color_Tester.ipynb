{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMH9Pc/M9ihvWxp4DdeTPxU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marioschlosser/llm-color-tester/blob/master/LLM_Color_Tester.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install python-dotenv"
      ],
      "metadata": {
        "id": "Ys4ArYJ7rO10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5m-2Z2DohNn"
      },
      "outputs": [],
      "source": [
        "# @title Initialize OpenAI\n",
        "import openai\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from itertools import islice\n",
        "import time\n",
        "import random\n",
        "import string\n",
        "import requests\n",
        "\n",
        "openai_key = \"\" # @param {type:\"string\"}\n",
        "\n",
        "openai.api_key = openai_key\n",
        "\n",
        "def get_response_message(response):\n",
        "    for choice in response.choices:\n",
        "        if choice.message.role == 'assistant':\n",
        "            message = choice.message.content\n",
        "    return message"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown The URL from which to load the prompt:\n",
        "url_prompt = \"https://raw.githubusercontent.com/marioschlosser/llm-color-tester/master/prompt.txt\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown The URL from which to load the objects and colors (format: object, color per line):\n",
        "url_objects = \"https://raw.githubusercontent.com/marioschlosser/llm-color-tester/master/objects_and_colors.txt\" # @param {type:\"string\"}\n",
        "\n",
        "# load the prompt text\n",
        "response_http = requests.get(url_prompt)\n",
        "if response_http.status_code == 200:\n",
        "    prompt = response_http.text\n",
        "else:\n",
        "    print(\"Failed to retrieve the prompt text file\")\n",
        "\n",
        "# initialize list of objects\n",
        "test_objects = {}\n",
        "\n",
        "response_http = requests.get(url_objects)\n",
        "if response_http.status_code == 200:\n",
        "    lines = response_http.text.split('\\n')\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            parts = line.split(\",\")\n",
        "            if len(parts) >= 2:\n",
        "                key = parts[0].strip()\n",
        "                value = parts[1].strip()\n",
        "                test_objects.update({key: value})\n",
        "else:\n",
        "    print(\"Failed to retrieve the text file\")\n",
        "\n",
        "# run the test for x different sizes of objects in the context window\n",
        "# @markdown Number of intervals to split the objects list into:\n",
        "num_test_intervals = 4 # @param {type:\"string\"}\n",
        "test_sizes = [len(test_objects) * (x+1) // num_test_intervals for x in range(num_test_intervals)]\n",
        "\n",
        "# number of complete tests to run\n",
        "# @markdown Number of test runs (for each test run, the object list will be shuffled):\n",
        "test_number_max = 3 # @param {type:\"string\"}\n",
        "\n",
        "# list of lists that will store the number of correct tests of each test run\n",
        "test_correct = []\n",
        "\n",
        "# current test index\n",
        "test_index = 0\n",
        "\n",
        "# maximum number of retries per OpenAI API call\n",
        "max_attempts = 5\n",
        "\n",
        "# create randomized string for test file name\n",
        "test_file_name = ''.join(random.choice(string.ascii_lowercase) for i in range(5))\n",
        "\n",
        "# create a text file that will store the number of correct tests of reach test run, with random file name\n",
        "with open(\"test_results_\" + test_file_name + \".txt\", \"w\") as text_file:\n",
        "    text_file.write(\"\")"
      ],
      "metadata": {
        "id": "yo6cPATFrrcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "id": "8hT87ZPJsTAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run several tests\n",
        "for test_index in range(test_number_max):\n",
        "    # shuffle the dict for each test run\n",
        "    tmp_items = list(test_objects.items())\n",
        "    random.shuffle(tmp_items)\n",
        "    shuffled_dict = dict(tmp_items)\n",
        "\n",
        "    # reset test results for this test run\n",
        "    test_correct.append([])\n",
        "\n",
        "    # test all the different context sizes\n",
        "    for test_size in test_sizes:\n",
        "        print(\"Now evaluating # of objects in context window: \" + str(test_size))\n",
        "\n",
        "        # create the input string of objects\n",
        "        data_string = \"\\n\".join(f\"{index}. {key}\" for index, (key, value) in enumerate(islice(shuffled_dict.items(), test_size), 1))\n",
        "\n",
        "        # and for backup, here with colors\n",
        "        data_string_with_colors = \"\\n\".join(f\"{index}. {key}, {value}\" for index, (key, value) in enumerate(islice(shuffled_dict.items(), test_size), 1))\n",
        "\n",
        "        for attempt in range(max_attempts):\n",
        "            print(\"Attempt: \", attempt)\n",
        "\n",
        "            message = [{\"role\": \"system\", \"content\": prompt}, {\"role\": \"user\", \"content\": data_string}]\n",
        "\n",
        "            print(\"Input (with colors):\")\n",
        "            print(data_string_with_colors)\n",
        "\n",
        "            try:\n",
        "                response = openai.ChatCompletion.create(\n",
        "                    model=\"gpt-4\",\n",
        "                    max_tokens=3000,\n",
        "                    temperature=0,\n",
        "                    messages=message)\n",
        "\n",
        "                break  # if the operation succeeds, exit the loop\n",
        "            except Exception as e:\n",
        "                print(f\"Attempt {attempt + 1} failed with error {e}. Retrying...\")\n",
        "                time.sleep(30)\n",
        "            else:\n",
        "                print(f\"Operation failed after {max_attempts} attempts.\")\n",
        "\n",
        "        msg = get_response_message(response)\n",
        "\n",
        "        print(\"Output:\")\n",
        "        print(msg)\n",
        "\n",
        "        # save input and output in a text file\n",
        "        with open(\"test\" + str(test_index) + \"_\" + str(test_size) + \"_\" + test_file_name + \".txt\", \"w\") as text_file:\n",
        "            text_file.write(\"Input (with colors):\\n\")\n",
        "            text_file.write(data_string_with_colors)\n",
        "            text_file.write(\"\\n\\nOutput:\\n\")\n",
        "            text_file.write(msg)\n",
        "\n",
        "        # Split the string by newline to get each object-color pair\n",
        "        pairs = msg.split(\"\\n\")\n",
        "\n",
        "        # Create a dictionary by splitting each pair by a comma\n",
        "        ranked_dict = {}\n",
        "        for pair in pairs:\n",
        "            try:\n",
        "                ranked_dict.update({pair.split(\",\")[0].strip(): pair.split(\",\")[1].strip()})\n",
        "            except:\n",
        "                print(\"error: \" + pair)\n",
        "\n",
        "        # evaluate the color correctness\n",
        "        correct = sum(1 for obj, color in islice(shuffled_dict.items(), test_size) if color == ranked_dict.get(obj, None))\n",
        "\n",
        "        test_correct[test_index].append(correct)\n",
        "\n",
        "        # append the test index, the size of the test and the number of correct results to the results text file\n",
        "        with open(\"test_results_\" + test_file_name + \".txt\", \"a\") as text_file:\n",
        "            text_file.write(str(test_index) + \",\" + str(test_size) + \",\" + str(correct) + \"\\n\")\n",
        "\n",
        "print(test_correct)"
      ],
      "metadata": {
        "id": "D25wzUTkqeE9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}